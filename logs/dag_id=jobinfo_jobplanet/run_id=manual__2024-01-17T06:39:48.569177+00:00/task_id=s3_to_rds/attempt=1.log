[2024-01-17T06:48:45.025+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: jobinfo_jobplanet.s3_to_rds manual__2024-01-17T06:39:48.569177+00:00 [queued]>
[2024-01-17T06:48:45.034+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: jobinfo_jobplanet.s3_to_rds manual__2024-01-17T06:39:48.569177+00:00 [queued]>
[2024-01-17T06:48:45.035+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2024-01-17T06:48:45.035+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 1
[2024-01-17T06:48:45.035+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2024-01-17T06:48:45.055+0000] {taskinstance.py:1300} INFO - Executing <Task(PythonOperator): s3_to_rds> on 2024-01-17 06:39:48.569177+00:00
[2024-01-17T06:48:45.072+0000] {standard_task_runner.py:55} INFO - Started process 30924 to run task
[2024-01-17T06:48:45.074+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'jobinfo_jobplanet', 's3_to_rds', 'manual__2024-01-17T06:39:48.569177+00:00', '--job-id', '4400', '--raw', '--subdir', 'DAGS_FOLDER/jobinfo_jobplanet.py', '--cfg-path', '/tmp/tmp_2g9ziir']
[2024-01-17T06:48:45.074+0000] {standard_task_runner.py:83} INFO - Job 4400: Subtask s3_to_rds
[2024-01-17T06:48:45.265+0000] {task_command.py:388} INFO - Running <TaskInstance: jobinfo_jobplanet.s3_to_rds manual__2024-01-17T06:39:48.569177+00:00 [running]> on host 565f6bcc10b6
[2024-01-17T06:48:45.448+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=jobinfo_jobplanet
AIRFLOW_CTX_TASK_ID=s3_to_rds
AIRFLOW_CTX_EXECUTION_DATE=2024-01-17T06:39:48.569177+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2024-01-17T06:39:48.569177+00:00
[2024-01-17T06:48:45.746+0000] {base.py:73} INFO - Using connection ID 'rds_conn' for task execution.
[2024-01-17T06:48:45.872+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/jobinfo_jobplanet.py", line 739, in s3_to_rds
    cur.execute(insert_query, tuple(row))
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/cursors.py", line 206, in execute
    res = self._query(query)
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/cursors.py", line 319, in _query
    db.query(q)
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/connections.py", line 254, in query
    _mysql.connection.query(self, query)
MySQLdb.ProgrammingError: (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '0, 1, 2, 3, 4, 5, 6, 7, 0.1, 0.1.1, 0.1.1.1, 0.1.1.1.1, 0.1.1.1.1.1, 0.1.1.1.1.1' at line 1")
[2024-01-17T06:48:45.880+0000] {taskinstance.py:1323} INFO - Marking task as FAILED. dag_id=jobinfo_jobplanet, task_id=s3_to_rds, execution_date=20240117T063948, start_date=20240117T064845, end_date=20240117T064845
[2024-01-17T06:48:45.899+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 4400 for task s3_to_rds ((1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '0, 1, 2, 3, 4, 5, 6, 7, 0.1, 0.1.1, 0.1.1.1, 0.1.1.1.1, 0.1.1.1.1.1, 0.1.1.1.1.1' at line 1"); 30924)
[2024-01-17T06:48:45.920+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2024-01-17T06:48:45.976+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
